---
title: "Report Airline Passenger Satisfaction"
author: "Kilian Sennrich"
date: "26 3 2021"
output: pdf_document
---

This is the report of Kilian Sennrich for the first assignment of the class "10616 Machine Learning" held by Prof. Dr. Dietmar Maringer (University of Basel). The task is to perform a complete analysis for one of the 10 datasets provided in the lecture. I chose the "Airline Passenger Satisfaction" dataset from kaggle: https://www.kaggle.com/teejmahal20/airline-passenger-satisfaction

From the Website:

# What factors lead to customer satisfaction for an Airline?

This dataset contains an airline passenger satisfaction survey. What factors are highly correlated to a satisfied (or dissatisfied) passenger? Can you predict passenger satisfaction?

Note that this dataset was modified from another dataset by John D on Kaggle. It has been cleaned up for the purposes of classification.

1. Gender: Gender of the passengers (Female, Male)
2. Customer Type: The customer type (Loyal customer, disloyal customer)
3. Age: The actual age of the passengers
4. Type of Travel: Purpose of the flight of the passengers (Personal Travel, Business Travel)
5. Class: Travel class in the plane of the passengers (Business, Eco, Eco Plus)
6. Flight distance: The flight distance of this journey
7. Inflight wifi service: Satisfaction level of the inflight wifi service (0:Not Applicable;1-5)
8. Departure/Arrival time convenient: Satisfaction level of Departure/Arrival time convenient
9. Ease of Online booking: Satisfaction level of online booking
10. Gate location: Satisfaction level of Gate location
11. Food and drink: Satisfaction level of Food and drink
12. Online boarding: Satisfaction level of online boarding
13. Seat comfort: Satisfaction level of Seat comfort
14. Inflight entertainment: Satisfaction level of inflight entertainment
15. On-board service: Satisfaction level of On-board service
16. Leg room service: Satisfaction level of Leg room service
17. Baggage handling: Satisfaction level of baggage handling
18. Check-in service: Satisfaction level of Check-in service
19. Inflight service: Satisfaction level of inflight service
20. Cleanliness: Satisfaction level of Cleanliness
21. Departure Delay in Minutes: Minutes delayed when departure
22. Arrival Delay in Minutes: Minutes delayed when Arrival
23. Satisfaction: Airline satisfaction level(Satisfaction, neutral or dissatisfaction)

# Introduction

This Report explains the analysis of the dataset. It is organized according to the main.R file, that contains the analysis. This report explains the thought process behind the analysis. Assumptions in the analysis are clearly documented here!!

# Table of contents

1. Inspection
2. Visualisation
3. Correlation Analysis



## Inspection, Balance, Missing values

A first look into the data revealed, that the data is pretty clean. There were 2 columns to be deleted (V1, id) that contain unique identifiers to single observations and that don't add value to the analysis, since we do not try to make statements about single customers. 
The data is generally well balanced, with approx. 50 percent of the criterion (satisfaction) in both groups. Therefore, in this regard no further steps are necessary.
Missing values (NAs and sometimes NULL objects) can only be upserved in the "Arrival Delay in Minutes" column. There are 310 missing values in total, that distributed approximately equally between the two attributes of the criterion. I had the hypothesis, that the 310 values might all originate from the same plane, since an average airplane could transport about that amount of passengers. To check this i made the following assumption **If the NA values originated from passengers of the same flight, the flight distance for all these passengers would be equivalent **. The hypothesis is wrong, since the max number of passengers that had the same flight distance was 5. 
In order to get good data for the classifier, I wanted to get rid of the NAs. For imputation I had three ideas in mind: replacement by mean, delete rows or MICE. Mice seemed an over-the-top approach for only 310 missing values. Both of the other approaches would work, but i chose to omit the NA rows, since the proportion of data that falls away is very small (0.2 %) and having to many identical values (mean imputation) in the data, sometimes affects the accuracy of the model.

## Visualisation

### Variable by Criterion

```{r include=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
source("..//code//packages.R")
X<-fread("..//data//train.csv")
X<-setDF(X)
X<-subset(X, select = -c(V1, id))
X<-na.omit(X)
for (i in 1:length(X)){
  if(is.character(X[,i])){
    X[,i]<-as.factor(X[,i])
  }
}

```


\begin{figure}
    \includegraphics[width=.24\textwidth]{..//plots//plot1.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot2.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot4.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot5.jpg}\hfill
    \caption{Vizualisation of all nominal variables by criterion}\label{fig:foobar}
\end{figure}

\hfill\break
\hfill\break

\begin{figure}
    \includegraphics[width=.24\textwidth]{..//plots//plot7.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot8.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot9.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot10.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot11.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot12.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot13.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot14.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot15.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot16.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot17.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot18.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot19.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot20.jpg}\hfill
    \caption{Vizualisation of all ordinal variables by criterion}\label{fig:foobar}
\end{figure}

\begin{figure}
    \includegraphics[width=.24\textwidth]{..//plots//plot3.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot6.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot21.jpg}\hfill
    \includegraphics[width=.24\textwidth]{..//plots//plot22.jpg}\hfill
    \caption{Vizualisation of all intervall variables by criterion}\label{fig:foobar}
\end{figure}

This is the text for the visualisation part,

### Correlation Analysis

\begin{figure}
  \includegraphics{..//plots//heatmap.jpg}
  \caption{The darker the color, the more significant the correlation. The correlation-coefficient is spearmans rho. Be careful with interpretation since there were nominal variables in there!}\label{fig:foobar}
\end{figure}

```{r echo=FALSE}
for(i in 1:length(X)){
  if(is.character(X[,i])){
    X[,i]<-as.factor(X[,i])
  }
}

#Gender: 1 = Female; 2 = Male
levels(X$Gender)<-c(1:2)

#Customer Type: 1 = disloyal Customer; 2 = Loyal Customer
levels(X$`Customer Type`)<-c(1:2)

#Type of Travel: 1 = Business travel, 2 = Personal Travel
levels(X$`Type of Travel`)<-c(1:2)

#Class: 1 = Business; 2 = Eco; 3 = Eco Plus                               
levels(X$Class)<-c(1:3)

#satisfaction: 1 = neutral or dissatisfied; 2 = satisfied
levels(X$satisfaction)<-c(1:2)

# make factors to integers again
for(i in 1:length(X)){
  if(is.factor(X[,i])){
    X[,i]<-as.numeric(X[,i])
  }
}
Xnozero<-X
for (i in 1:length(X)) {
  Xnozero[,i]<-ifelse(Xnozero[,i] == 0, NA, Xnozero[,i])
}

yxcorrelation<-list()
for(i in 1:(length(Xnozero)-1)){
  yxcorrelation[[i]]<-data.table(y = "satisfaction",
                                 x = names(Xnozero)[i],
                                 cor = cor(y = Xnozero$satisfaction,
                                           x = Xnozero[,i],
                                           method = "spearman",
                                           use = "complete.obs"))
 
}
#make list a data.table
yxcorrelation<-rbindlist(yxcorrelation)

#check correlation with cohens interpretation criteria

  # correlations higher than 0.5
  yxcorrelation[which(cor > 0.5 | cor < -0.5)] %>% kable(caption = "correlations higher than +-0.5")
    
  # correlations between 0.3 and 0.5
  yxcorrelation[which(cor>0.3 & cor<0.5 | cor < -0.3 & cor > -0.5)] %>% kable(caption = "correlations between +-0.3 and +-0.5")
    
  # correlations between 0.1 and 0.3
  yxcorrelation[which(cor > 0.1 & cor < 0.3 | cor < -0.1 & cor > -0.3)] %>% kable(caption = "correlations between +-0.1 and +-0.3")
  
   # items not associated with criterion
  yxcorrelation[which(cor < 0.1 & cor > -0.1)] %>% kable(caption = "items not associated with criterion (-0.1 and 0.1)")
```

Bad idea to use cor. test since we had to correct with bonferroni and high samplesize would make everything significant 

Why not leave uncorrelated items out?
Bc we use random forrest classifier which does not care about uncorrelated items

why leave out 0s in the satisfaction ratings? it might lead to biased correlation estimates, since spearman assumes at least ordinal niveau. limitation with nominal niveau but see visualisations. 
bc it can still give some insight to the data to know which people do not have an opinion
